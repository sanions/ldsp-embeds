<html>
<head>
	<link rel="stylesheet" href="hoverEffect.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="hoverEffect.js"></script>
<style>
	/* Styling for highlighting */
	.hover-highlight {
		position: relative;
		cursor: pointer;
	}
	.hover-highlight:hover .highlight-text {
		display: block;
	}
	.highlight-text {
		display: none;
		position: absolute;
		background: rgba(0, 0, 0, 0.8);
		color: #fff;
		padding: 5px;
		border-radius: 3px;
		top: -25px;
		left: 0;
		white-space: nowrap;
	}
</style>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 10px 10px 10px 10px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	.sub-content-block {
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		/* border-left: 1px solid #DDD;
		border-right: 1px solid #DDD; */
		padding: 10px 0px 0px 0px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 26px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	h2 {
		font-size: 20px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	h3 {
		font-size: 17px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>

	  <title>The Platonic Representation Hypothesis</title>
      <meta property="og:title" content="The Platonic Representation Hypothesis" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">A Grand Unified Theory of Deep Learning</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="your_website">Your name</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="your_partner's_website">Your partner's name</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <!-- <a href="#does_x_do_y">Does X do Y?</a><br><br> -->
			  <a href="#lit-review">Literature Review</a><br><br>
			  <a href="#data">Data</a><br><br>
			  <a href="#experiments">Experiments</a><br><br>
			  <!-- <a href="#mean-shift">Mean-shift</a><br><br> -->
              <a href="#implications_and_limitations">Implications and limitations</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
            <img src="./images/your_image_here.png" width=512px/>
		    </div>
		    <div class="margin-right-block">
						Caption for the image.
		    </div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>
						Sentence embeddings, derived from transformer-based models like BERT, encode semantic and syntactic properties into high-dimensional vectors. While these embeddings have revolutionized NLP by enabling tasks like sentiment analysis and paraphrase detection, their interpretability remains a major challenge. Existing research focuses on probing static embeddings to reveal latent properties, but little is understood about how linguistic relationships—such as tense shifts, negation, or synonymy—are dynamically encoded and manipulated in embedding spaces.
						<br>
						<br>
						Our work addresses this gap by focusing on the generation of embeddings for Sentence 2 from the embedding of Sentence 1, where Sentence 2 represents a linguistically transformed version of Sentence 1. This generative approach allows us to analyze how specific linguistic relationships are encoded as shifts in embedding dimensions and evaluate how well these transformations align with ground-truth embeddings. We move beyond passive probing to actively generating embeddings for  relational transformations, offering deeper insights into the inner workings of these models.
						<br>
						<br>
						This project contributes new knowledge to NLP and deep learning by uncovering the mechanisms underlying relational encoding in embedding spaces. Specifically, we investigate which dimensions encode linguistic relationships, how effectively embeddings can approximate transformations, and whether embedding spaces can be directly manipulated to reflect controlled changes. These findings bridge the gap between interpretability and generative capabilities, providing both theoretical insights and practical tools for embedding manipulation.
						<br>
						<br>
						The generative approach expands the scope of embedding analysis by revealing how embeddings dynamically encode linguistic transformations, not just whether they capture properties. This has implications for explainable AI, enabling transparent and interpretable manipulations of embeddings to reflect specific linguistic changes. Additionally, generating embeddings for transformations introduces fine-grained control in NLP systems, with applications in controlled text generation, robust paraphrasing, and personalized language tasks. By addressing the underexplored dynamic nature of embeddings, our work provides a foundation for building more robust, interpretable, and actionable embedding-based systems.						
		    </div>
		    <div class="margin-right-block">
						Margin note that clarifies some detail #main-content-block for intro section.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
			<div class="main-content-block" id="lit-review">
			<h1>Literature Review</h1>
			Insert content here. Make sure to do in-text citations and link to bib.
			</div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
			<div class="main-content-block" id="data">
			<h1>Data</h1>
			<p>
				These efforts reflect the ongoing commitment to making embeddings more interpretable and addressing the inherent opacity of high-dimensional representations. 
				However, while these studies offer critical insights, a persistent challenge lies in enhancing the interpretability of widely adopted techniques such as 
				Word2Vec, GloVe, and BERT without requiring alternative architectures or training paradigms.
			</p>
			<p>
				To address the challenges in understanding how embeddings encode linguistic properties, in this study, we introduce the Linguistically Distinct Sentence Pairs (LDSP) dataset. 
				Based on this dataset, we propose dimension-wise analysis methods to identify the embedding dimensions most responsive to specific linguistic features 
				and introduce a novel metric, the <b>Embedding Dimension Impact (EDI)</b> score, to quantify their significance. 
				We conduct a comprehensive analysis of BERT embeddings to demonstrate the effectiveness of our dimension-wise analysis methods 
				and the utility of the EDI score in uncovering meaningful linguistic encoding distributions.
			</p>
			
			<h2>Dataset overview</h2>
			<p>
				We curated a dataset of 1000 LDSPs for each of the 10 linguistic properties we wanted to investigate. 
				The dataset was generated using Google's <code>gemini-1.5-flash</code> model API. 
				This model was selected due to its reliability and cost-efficiency while being able to produce consistent outputs across a variety of linguistic contexts. 
				The model was prompted with a set of reference LDSPs as well as a description of the linguistic property to ensure high-quality outputs. 
				These outputs were generated in batches of 100 LDSPs at a time.
			</p>
			
			<h2>Linguistic properties</h2>
			<p>
				The linguistic properties tested were chosen to explore various semantic and syntactic relationships. 
				Below is a summary of each property from the LDSP dataset:
			</p>
		
			<table border="1" align=center cellspacing="0" cellpadding="10" width="50%">
				<thead>
					<tr>
						<th>Linguistic property</th>
						<th>Description</th>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td><i>Control</i></td>
						<td>Contains completely unrelated sentence pairs for comparison.</td>
					</tr>
					<tr>
						<td><i>Synonym</i></td>
						<td>Both sentences have the same meaning, with one word replaced by its synonym.</td>
					</tr>
					<tr>
						<td><i>Quantity</i></td>
						<td>Switch from an exact number to a grouping word.</td>
					</tr>
					<tr>
						<td><i>Tense</i></td>
						<td>One sentence is constructed in the present tense, while the other is in the past tense.</td>
					</tr>
					<tr>
						<td><i>Intensifier</i></td>
						<td>Degree of emphasis present within a sentence.</td>
					</tr>
					<tr>
						<td><i>Definiteness</i></td>
						<td>Use of definite or indefinite articles within a sentence, such as <i>the</i> compared to <i>a</i>.</td>
					</tr>
					<tr>
						<td><i>Factuality</i></td>
						<td>The degree of truth implied by the structure of the sentence.</td>
					</tr>
					<tr>
						<td><i>Polarity</i></td>
						<td>Similar to negation but occurs when an antonym is used to reverse the meaning of the sentence completely.</td>
					</tr>
					<tr>
						<td><i>Negation</i></td>
						<td>Addition of <i>not</i> to a sentence, negating the meaning.</td>
					</tr>
				</tbody>
			</table>
			<br>
			<p>
				These linguistic properties were selected to provide a comprehensive view of both semantic and syntactic relationships in embedding spaces.
				Additionally, the inclusion of the <i>control</i> category ensures that results are contextualized against unrelated sentences, offering a baseline for comparison.
				The interactive graphic below shows examples of Sentence 1 and Sentence 2 according to their linguistic relationship.
			</p>
			<div class="image-container">
				<div class="image-wrapper" data-coordinates="
				752,182,1167,259,image1.jpg;
				752,262,1167,339,image2.jpg;
				752,342,1167,419,image3.jpg;
				752,422,1167,499,image4.jpg;
				752,502,1167,579,image5.jpg;
				752,582,1167,659,image6.jpg;
				752,662,1167,739,image7.jpg;
				752,742,1167,819,image8.jpg;
				752,822,1167,899,image9.jpg;
				752,902,1167,979,image10.jpg">
				<img src="images/linguistic_prop/linguistic_prop.001.jpeg" alt="Default">
				<img src="images/linguistic_prop/linguistic_prop.002.jpeg" alt="Image 1">
				<img src="images/linguistic_prop/linguistic_prop.003.jpeg" alt="Image 2">
				<img src="images/linguistic_prop/linguistic_prop.004.jpeg" alt="Image 3">
				<img src="images/linguistic_prop/linguistic_prop.005.jpeg" alt="Image 4">
				<img src="images/linguistic_prop/linguistic_prop.006.jpeg" alt="Image 5">
				<img src="images/linguistic_prop/linguistic_prop.007.jpeg" alt="Image 6">
				<img src="images/linguistic_prop/linguistic_prop.008.jpeg" alt="Image 7">
				<img src="images/linguistic_prop/linguistic_prop.009.jpeg" alt="Image 8">
				<img src="images/linguistic_prop/linguistic_prop.010.jpeg" alt="Image 9">
				<img src="images/linguistic_prop/linguistic_prop.011.jpeg" alt="Image 10">
				</div>
			</div>
			</div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block" id="experiments">
            <h1>Experiments</h1>
			The task of generating Sentence 2 embeddings based on Sentence 1 embeddings involves modeling the shifts observed in embedding dimensions as they encode linguistic transformations. Two approaches are employed in this project: mean-shift and regression-based transformations. These methods rely on training data (80% of the dataset) to identify patterns and apply those transformations to the test data (remaining 20%).
			<!-- <br> -->
			<div class="sub-content-block" id="sampling">
				<h2>Sampling</h2>
				<p>
					The sampling-based method selectively alters certain dimensions based on their 
					EDI scores. This method ensures that only the most linguistically significant dimensions (as determined by their EDI scores) 
					are modified, while the less significant dimensions remain unchanged. This approach leverages the statistical properties of Sentence 2 embeddings 
					to generate more linguistically meaningful transformations.
				</p>
				<!-- <h3>Methodology</h3> -->
				<p>
					For each dimension in the embedding, its EDI score is compared against a predefined threshold (default of 0.75). Dimensions with EDI scores below the threshold 
					are left unchanged, while those with EDI scores greater than or equal to the threshold are sampled from a Gaussian distribution derived from the 
					corresponding dimension's mean and standard deviation in Sentence 2 embeddings.
				</p>
				<p>
					This selective approach means that we still incorporate the original Sentence 1 embedding into the transformation process. If we were to sample for all dimensions, the original Sentence 1 embedding would not contribute to the generation of Sentence 2 embeddings, defeating the purpose of using Sentence 1 as a reference.
				</p>
				<!-- <h3>Mathematical representation</h3> -->
				<p>
					For a given embedding \( e \), the transformed embedding \( e' \) is generated as follows:
				</p>
				<p>
					\[
					e'[d] = 
					\begin{cases} 
						e[d], & \text{if } \text{EDI}[d] < \text{threshold} \\
						\text{Sample from } \mathcal{N}(\mu_{s2}[d], \sigma_{s2}[d]), & \text{if } \text{EDI}[d] \geq \text{threshold}
					\end{cases}
					\]
				</p>
				<p>
					Here:
				</p>
				<ul>
					<li>\( e[d] \): The \( d \)-th dimension of the original embedding.</li>
					<li>\( \text{EDI}[d] \): The EDI score for the \( d \)-th dimension.</li>
					<li>\( \mathcal{N}(\mu_{s2}[d], \sigma_{s2}[d]) \): Gaussian distribution with mean \( \mu_{s2}[d] \) and standard deviation \( \sigma_{s2}[j] \) 
					derived from Sentence 2 embeddings.</li>
					<li>\( \text{threshold} \): The EDI score threshold.</li>
				</ul>
				<!-- <h3>Significance</h3> -->
				<!-- <p>
					This approach highlights the role of highly impactful embedding dimensions in linguistic transformations, while avoiding unnecessary modifications 
					to dimensions with lower EDI scores. By focusing on sampling from the natural distribution of Sentence 2 embeddings, the sampling-based method 
					ensures that the generated embeddings maintain linguistic relevance and statistical integrity.
				</p> -->
			</div>
			
				<div class="sub-content-block" id="mean-shift">
				<h2>Mean-shift</h2>
					<!-- <p> -->
						The mean-shift approach simplifies the problem by assuming that the transformation for a given linguistic property 
						can be modeled as a fixed, dimension-specific shift. These shifts are calculated from the training data and then 
						applied to test data.
					<!-- </p> -->
					<ol>
						<li>
							<h3>Calculating mean shifts</h3>
							<p>
								For each dimension \( d \) in the embedding space, the mean shift \( \Delta_d \) is calculated as the difference 
								between the average value of that dimension for Sentence 2 embeddings and Sentence 1 embeddings in the training data.
							</p>
							<p>Mathematically:</p>
							<p>
								\[
								\Delta_d = \frac{1}{n} \sum_{i=1}^n E_2[i, d] - \frac{1}{n} \sum_{i=1}^n E_1[i, d]
								\]
								<!-- <span class="highlight-text">This represents the mean shift for dimension \( d \).</span> -->
							</p>
							<p>Here:</p>
							<ul>
								<li>
									\( E_1[i, d] \): Represents the \( d \)-th dimension of the \( i \)-th Sentence 1 embedding.
								</li>
								<li>
									\( E_2[i, d] \): Represents the \( d \)-th dimension of the \( i \)-th Sentence 2 embedding.
								</li>
								<li>
									\( n \): Number of sentences in the training set.
								</li>
							</ul>
							<p>
								The result is a vector \( \Delta = [\Delta_1, \Delta_2, \dots, \Delta_d] \), where \( d \) is the embedding dimensionality.
							</p>
						</li>
						<li>
							<h3>Applying mean shifts</h3>
							<p>
								To generate the embedding for Sentence 2 during testing, the calculated shifts \( \Delta \) are added to the corresponding
								dimensions of the Sentence 1 embedding:
							</p>
							<p class="hover-highlight">
								\[
								e_2'[d] = e_1[d] + \Delta_d \quad \forall d \in \{1, \dots, D\}
								\]
								<span class="highlight-text">This adjusts Sentence 1 embeddings with the mean shifts.</span>
							</p>
						</li>
					</ol>
					<p>
						This method is efficient and captures global trends in dimension shifts, making it particularly useful for transformations 
						where shifts are consistent across examples.
					</p>
				</div>

				<div class="sub-content-block" id="regression">
					<h2>Logistic Regression</h2>
					<p>
						While the mean-shift approach captures global trends, it does not account for variability or nonlinear relationships 
						within specific dimensions. The regression-based approach overcomes this limitation by learning dimension-specific 
						logistic regression models to predict changes.
					</p>
					<ol>
						<li>
							<h3>Learning dimension transformations</h3>
							<p>
								For each dimension \( d \), a logistic regression model is trained using the \( d \)-th dimension of Sentence 1 embeddings as input 
								and the corresponding dimension of Sentence 2 embeddings as output:
							</p>
							<p>
								\[
								E_2[i, d] = f_d(E_1[i, d]) + \epsilon
								\]
							</p>
							<p>
								where \( f_d(x) = \frac{1}{1 + e^{-(w_d x + b_d)}} \) is the sigmoid function parameterized by the weight \( w_d \) and bias \( b_d \), 
								and \( \epsilon \) is the residual error.
							</p>
							<p>
								The parameters \( w_d \) and \( b_d \) are learned by minimizing the negative log-likelihood (NLL) loss over the training set:
							</p>
							<p>
								\[
								\min_{w_d, b_d} -\frac{1}{n} \sum_{i=1}^n \left( E_2[i, d] \log(f_d(E_1[i, d])) + (1 - E_2[i, d]) \log(1 - f_d(E_1[i, d])) \right)
								\]
							</p>
						</li>
						<li>
							<h3>Generating regression-based embeddings</h3>
							<p>
								For a test embedding \( e_1 \), the logistic regression models are applied to each dimension to predict the corresponding dimensions of Sentence 2:
							</p>
							<p>
								\[
								e_2'[d] = f_d(e_1[d]) = \frac{1}{1 + e^{-(w_d e_1[d] + b_d)}} \quad \forall d \in \{1, \dots, D\}
								\]
							</p>
						</li>
					</ol>
					<p>
						This approach allows each dimension to respond differently to the linguistic transformation, making it more flexible than the mean-shift method.
					</p>
				</div>

				<div class="sub-content-block" id="edi-cosine-loss">
					<h2>EDI + Cosine Loss</h2>
					<p>
						The EDI + Cosine Loss method introduces a weighted transformation loss that selectively penalizes changes to embedding dimensions based on 
						their EDI scores. By combining a dimension-specific weighting mechanism with cosine similarity, this method 
						ensures that embedding transformations preserve linguistic meaning while penalizing unnecessary deviations.
					</p>
					<ol>
						<li>
							<h3>Methodology</h3>
							<p>
								For each dimension \( d \) in the embedding, the EDI score determines its contribution to the transformation loss. Dimensions with high EDI scores 
								are considered linguistically significant and are assigned lower penalties for changes, encouraging the model to focus on altering dimensions 
								that encode meaningful transformations. Dimensions with low EDI scores, on the other hand, are penalized more heavily to prevent unnecessary changes.
							</p>
							<p>
								The EDI scores are scaled using a sigmoid function, defined as:
							</p>
							<p>
								\[
								\text{EDI}_d = \text{Sigmoid}(5 \cdot (\text{EDI}[d] - 0.5))
								\]
							</p>
							<p>
								Here:
							</p>
							<ul>
								<li>\( \text{EDI}[d] \): The EDI score for the \( d \)-th dimension.</li>
								<li>\( \text{EDI}_d \): The scaled weight for the \( d \)-th dimension, emphasizing high EDI scores and de-emphasizing low scores.</li>
							</ul>
							<br>
						</li>
						<li>
							<h3>Loss calculation</h3>
							<p>
								The EDI + Cosine Loss combines dimension-specific penalties and a similarity measure. The total loss is computed as:
							</p>
							<p>
								\[
								\mathcal{L} = \frac{1}{D} \sum_{d=1}^D \left( 1 - \text{EDI}_d \right) \cdot |e_1[d] - e_2[d]| \cdot (1 - \text{sim}(e_1, e_2))
								\]
							</p>
							<p>
								Here:
							</p>
							<ul>
								<li>\( d \): The total number of dimensions in the embedding.</li>
								<li>\( e_1[d] \): The \( d \)-th dimension of the original embedding (Sentence 1).</li>
								<li>\( e_2[d] \): The \( d \)-th dimension of the transformed embedding (Sentence 2).</li>
								<li>\( \text{EDI}_d \): The scaled EDI score for the \( d \)-th dimension, derived using the chosen scaling method.</li>
								<li>\( \text{sim}(e_1, e_2) \): The <i>cosine similarity</i> between the original and transformed embeddings, i.e., cosine distance between two vectors the cosine distance between two vectors.</li>
							</ul>
							<p>
								By combining the weighted dimension-wise changes and cosine similarity, this loss function encourages transformations that preserve 
								overall semantic similarity while prioritizing changes in dimensions with high EDI scores.
							</p>
						</li>
					</ol>
					<p>
						The EDI + Cosine Loss method is particularly effective in tasks where preserving the semantic meaning of embeddings is critical. 
						By weighting dimensions based on their linguistic significance (EDI scores), the transformations are both targeted and interpretable. 
						Furthermore, the integration of cosine similarity aligns the embeddings globally, enabling the method to balance local changes with overall structural coherence.
					</p>
				</div>

				<div class="sub-content-block" id="contrastive-loss">
					<h2>Contrastive Loss Methods</h2>
					<p>
						The contrastive loss methods introduce a robust objective that evaluates embedding transformations by comparing them against both positive and negative pairs. 
						The goal is to minimize the distance between the transformed embedding and the original embedding (positive pair) while maximizing the distance between 
						the transformed embedding and a randomly sampled negative embedding (negative pair). These methods encourage meaningful transformations that preserve 
						semantic consistency while separating embeddings from unrelated contexts.
					</p>
					<ol>
						<li>
							<h3>Contrastive loss objective</h3>
							<p>
								For a transformed embedding \( e_2' \), the contrastive loss is defined as:
							</p>
							<p>
								\[
								\mathcal{L}_{\text{contrastive}} = \text{dist}(e_2', e_1) + \max \left( 0, \text{margin} - \text{dist}(e_2', e_{\text{neg}}) \right)
								\]
							</p>
							<p>
								Here:
							</p>
							<ul>
								<li>\( e_2' \): The transformed embedding (Sentence 2).</li>
								<li>\( e_1 \): The original embedding (Sentence 1).</li>
								<li>\( e_{\text{neg}} \): A randomly sampled negative embedding (unrelated sentence).</li>
								<li>\( \text{margin} \): A predefined margin for sufficient separation from negative examples (default is 1).</li>
								<li>\( \text{dist}(x, y) \): A distance function that measures the similarity or dissimilarity between two embeddings.</li>
							</ul>
						
							<p>The distance function \( \text{dist}(x, y) \) can be one of the following:</p>
							<ul>
								<li><b>Mean squared error (MSE):</b> Measures the squared Euclidean distance:
									\[
									\text{dist}(x, y) = \|x - y\|^2
									\]
								</li>
								<li><b>Cosine similarity:</b> Measures the cosine distance between two vectors:
									\[
									\text{dist}(x, y) = 1 - \text{sim}(x, y)
									\]
								</li>
							</ul>
						</li>
						<li>
							<h3>Combined loss</h3>
							<p>
								For robust embedding transformations, the contrastive loss can be combined with the transformation loss as follows:
							</p>
							<p>
								\[
								\mathcal{L} = \lambda_1 \cdot \mathcal{L}_{\text{transformation}} + \lambda_2 \cdot \mathcal{L}_{\text{contrastive}},
								\]
							</p>
							<p>
								where \( \lambda_1 \) = 1 = \( \lambda_2 \), i.e., the losses are equally weighted.
							</p>
						</li>
					</ol>
					<!-- <h3>Significance</h3> -->
					<p>
						The contrastive loss methods provide an additional layer of robustness by explicitly introducing a negative sampling objective. 
						By minimizing the distance to the original embedding and maximizing the distance from negative examples, we aim to have 
						transformed embeddings maintain their semantic relevance while remaining distinguishable from unrelated contexts. The combined 
						loss further integrates these objectives, balancing the preservation of linguistic properties with enhanced robustness against noise.
					</p>
				</div>
				
					

			
			<!-- <ol>
				<li>Mean-shift</li>
				<li>Regression</li>
				<li>Milk</li>
			</ol> -->

            In this section we embed a video:
						<video class='my-video' loop autoplay muted style="width: 725px">
								<source src="./images/mtsh.mp4" type="video/mp4">
						</video>
		    </div>
		    <div class="margin-right-block">
					A caption for the video could go here.
		    </div>
		</div>

		<div class="content-margin-container" id="implications_and_limitations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Implications and limitations</h1>
						Let's end with some discussion of the implications and limitations.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave">Allegory of the Cave</a>, Plato, c. 375 BC<br><br>
							<a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>

	<script src="hoverEffect.js"></script>

	</body>

</html>
