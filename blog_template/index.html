<html>
<head>
	<link rel="stylesheet" href="hoverEffect.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="hoverEffect.js"></script>
<style>
	/* Styling for highlighting */
	.hover-highlight {
		position: relative;
		cursor: pointer;
	}
	.hover-highlight:hover .highlight-text {
		display: block;
	}
	.highlight-text {
		display: none;
		position: absolute;
		background: rgba(0, 0, 0, 0.8);
		color: #fff;
		padding: 5px;
		border-radius: 3px;
		top: -25px;
		left: 0;
		white-space: nowrap;
	}
</style>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	/*
	.main-content-block {
		width: 80%;
    max-width: 1400px;
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 10px 10px 10px 10px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	*/

	.main-content-block {
		width: 80%; /* Change this percentage as needed */
    max-width: 1400px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}

	.sub-content-block {
    max-width: 1400px; /* Optional: Maximum width */
		background-color: #fff;
		/* border-left: 1px solid #DDD;
		border-right: 1px solid #DDD; */
		padding: 10px 0px 0px 0px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}
	
	
	/* .margin-left-block {
			font-size: 14px;
			width: 12%;
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			padding: 5px;
	} */

	.margin-left-block {
			font-size: 14px;
			width: 25%;
			max-width: 130px;
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			padding: 5px;
	}

	/* .margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			font-size: 14px;
			width: 15%; 
			max-width: 128px;
			position: relative;
			text-align: left;
			padding: 10px; 
	} */

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 26px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	h2 {
		font-size: 20px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	h3 {
		font-size: 17px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>

	  <title>6.7960 Final Project</title>
      <meta property="og:title" content="6.7960 Final Project" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container" id="intro">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
			<br>
			<table class="header" align=left>
				<tr>
					<td colspan=4>
						<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">A Grand Unified Theory of Deep Learning</span>
					</td>
				</tr>
				<tr>
						<td align=left>
								<span style="font-size:17px"><a href="your_website">Your name</a></span>
								<br>
								<span style="font-size:17px"><a href="your_website">Your email</a></span>
						</td>
						<td align=left>
								<span style="font-size:17px"><a href="your_partner's_website">Your partner's name</a></span>
								<br>
								<span style="font-size:17px"><a href="your_partner's_website">Your partner's email</a></span>
						</td>
				<tr>
					<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
				</tr>
		</table>
		
		</div>
		<!-- <div class="margin-right-block">
					Margin note that clarifies some detail #main-content-block for intro section.
		</div> -->
		<br>
		
	</div>
	<!-- <br> -->
	
	<!-- STRIP!!! -->
	<!-- <div style="height: 20px; width: 100%; color: blue; border-top: 1px #dddddd; border-bottom: 1px #dddddd;"></div> -->

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <!-- <a href="#does_x_do_y">Does X do Y?</a><br><br> -->
			  <a href="#related-work">Related work</a><br><br>
			  <a href="#data">Data</a><br><br>
			  <a href="#experiments">Experiments</a><br><br>
			  <!-- <a href="#mean-shift">Mean-shift</a><br><br> -->
			  <a href="#training">Training</a><br><br>
			  <a href="#eval">Evaluation</a><br><br>
			  <a href="#results">Results</a><br><br>
              <a href="#discussion">Discussion</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
			<br>
			<br>
            <img src="images/methods_diagram.jpeg" width="85%" id="figure1">
			<br>
			<p style="text-align: center; font-size: 14px">Figure 1. Methods diagram; see <a href="#data">Data</a> and <a href="#experiments">Experiments</a> for more information.</p>

			<!-- <div class="container">
				<img src="images/methods_diagram.jpeg" width="85%" id="figure1">
				<div class="caption">
					Figure 1. Methods diagram: Add your description or explanation here.
				</div>
			</div> -->
			<br>
			<br>
		    </div>
			<!-- <div class="margin-right-block">
				Figure 1. Methods diagram; see <a href="#data">Data</a> and <a href="#experiments">Experiments</a> for more information.
			</div> -->
		    <!-- <div class="margin-right-block">
						Caption for the image.
		    </div> -->
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>
						Sentence embeddings, derived from transformer-based models like BERT, encode semantic and syntactic properties into high-dimensional vectors. While these embeddings have revolutionized NLP by enabling tasks like sentiment analysis and text summarization, their interpretability remains a major challenge. Existing research focuses on probing static embeddings to reveal latent properties, but less is understood about how linguistic relationships—such as tense shifts, negation, or synonymy—are dynamically encoded and manipulated in embedding spaces.
						<br>
						<br>
						Our work addresses this gap by focusing on generating embeddings for a linguistically transformed version of a sentence, starting from the embedding of the original sentence. This generative approach enables us to analyze how specific linguistic relationships are captured as shifts in embedding dimensions and assess how closely these transformations align with ground-truth embeddings. 
						We move beyond passive probing to actively generating embeddings for  relational transformations.
						<br>
						<br>
						This project contributes new knowledge to NLP and deep learning by studying the mechanisms underlying relational 
						linguistic encoding in embedding spaces. Specifically, we investigate which dimensions encode linguistic relationships, 
						how effectively embeddings can approximate transformations, and whether embedding spaces can be directly manipulated to 
						reflect controlled changes. Our work augments interpretability research with generative capabilities, 
						providing both theoretical and practical insights for embedding manipulation.
						<br>
						<br>
						Our generative approach expands the scope of embedding analysis by revealing how embeddings 
						dynamically encode linguistic properties. This has implications for explainable AI (xAI), 
						enabling transparent and interpretable manipulations of embeddings to reflect specific linguistic changes. 
						Additionally, generating embeddings for transformations introduces fine-grained control in NLP systems, 
						with applications in controlled text generation and personalized language tasks. 
						By addressing the underexplored dynamic nature of embeddings, our work provides a foundation for building 
						more robust, interpretable, and actionable embedding-based systems.						
		    </div>
		    <!-- <div class="margin-right-block">
						Margin note that clarifies some detail #main-content-block for intro section.
		    </div> -->
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
			<div class="main-content-block" id="related-work">
			<h1>Related work</h1>
			<p>
				The development of sentence embeddings, particularly through transformer-based models such as BERT 
				(<a href="#ref_1">[1]</a>), has significantly advanced NLP. These embeddings enable models 
				to capture rich semantic and syntactic features. However, the interpretability of these embeddings remains a 
				persistent challenge, particularly in understanding how embeddings encode dynamic linguistic transformations. 
				This section reviews advancements in interpretable embeddings, representation analysis, and generative 
				embedding methods, situating our work within the identified research gaps.
			</p>
	
			<h3>Interpretable embeddings</h3>
			<p>
				Efforts to improve the interpretability of embeddings have focused on creating sparse or semantically aligned 
				representations. Early work by Murphy et al. (<a href="#ref_2">[2]</a>) utilized non-negative matrix factorization 
				(NMF) to generate interpretable embeddings from word co-occurrence matrices, ensuring dimensions corresponded 
				to distinct semantic features. Building on this, Sun et al. (<a href="#ref_3">[3]</a>) 
				modified the GloVe loss function to enforce sparsity and non-negativity, enhancing the interpretability of embeddings 
				while preserving their effectiveness for downstream tasks.
			</p>
			<p>
				Sparse representations have gained prominence as a means of improving interpretability. Faruqui et al. (<a href="#ref_5">[4]</a>) 
				proposed Sparse Overcomplete Word Vectors (SPOWV), which employed dictionary learning to produce sparse and semantically 
				meaningful embeddings. Similarly, Subramanian et al. (<a href="#ref_6">[5]</a>) introduced SPINE (SParse Interpretable Neural Embeddings), 
				leveraging k-sparse denoising autoencoders to create embeddings with explicit semantic alignment. These methods make embeddings more interpretable, 
				with individual dimensions corresponding to specific linguistic features. However, their focus is primarily on word embeddings and does not generalize 
				well to sentence-level representations.
			</p>
			<p>
				Another avenue involves fine-grained semantic embeddings. Panigrahi et al. (<a href="#ref_7">[6]</a>) introduced Word2Sense, 
				using Latent Dirichlet Allocation (LDA) to align embedding dimensions with word senses, enabling nuanced semantic analysis. 
				While these methods succeed in creating interpretable static embeddings, they do not address how embeddings respond to dynamic 
				linguistic transformations, such as tense shifts or paraphrasing.
			</p>
	
			<h3>Understanding representations in existing models</h3>
			<p>
				Probing tasks have been widely adopted to analyze how linguistic properties are encoded in embeddings. Conneau et al. 
				(<a href="#ref_8">[7]</a>) introduced a probing framework to evaluate sentence embeddings on ten linguistic properties, 
				including tense, agreement, and semantic roles. This approach provided valuable insights into embeddings' static capacities 
				but left open questions about how embeddings dynamically encode transformations.
			</p>
			<p>
				Hewitt and Manning (<a href="#ref_9">[8]</a>) extended this analysis to syntactic structures, demonstrating that syntactic 
				trees can be approximated through linear transformations of embedding spaces. Similarly, Senel et al. (<a href="#ref_10">[9]</a>) 
				showed that specific embedding dimensions align with semantic categories, revealing how embeddings reflect relational semantics. 
				While informative, these studies remain focused on static representations, overlooking the dynamic nature of linguistic transformations.
			</p>
			<p>
				Recent approaches have explored deeper conceptual insights into embeddings. Simhi and Markovitch (<a href="#ref_11">[10]</a>) 
				proposed transforming latent embedding spaces into interpretable conceptual spaces, enabling dynamic granularity in analyzing embedding dimensions. 
				These methods offer promising tools for interpreting embeddings but do not explicitly address transformations between embeddings.
			</p>
	
			<h3>Generative approaches and embedding transformations</h3>
			<!-- <a href="#figure1">Figure 1</a> -->
			<p>
				Probing embeddings through generative methods has gained traction as an active means of analysis. Kerscher and Eger 
				(<a href="#ref_12">[11]</a>) introduced Vec2Sent, a framework for generating sentences from embeddings to probe their semantic fidelity. 
				While useful for analyzing embeddings' static properties, this method does not investigate transformations at the embedding level, 
				such as generating embeddings for paraphrased sentences.
			</p>
			<p>
				Transformations in embedding spaces have been explored in task-specific contexts. Huang et al. (<a href="#ref_13">[12]</a>) 
				introduced a compositional framework for interpretable sentence embeddings, demonstrating how transformations between embeddings 
				correspond to semantic operations. Their method highlights the potential for analyzing relational transformations in embeddings 
				but is limited to specific compositional operations.
			</p>
			</div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
				<!-- <div class="margin-right-block"></div>
				Table 1. Linguistic properties.
			</div> -->
			<div class="main-content-block" id="data">
			<h1>Data</h1>
			<p>
				The <b>Linguistically Distinct Sentence Pairs (LDSP)</b> dataset forms the foundation for our study, providing a diverse set of linguistic transformations 
				critical for understanding how embeddings encode semantic and syntactic relationships. The LDSP dataset, introduced in prior work (<a href="#ref_14">[13]</a>), contains sentence pairs 
				that differ along specific linguistic properties shown in <a href="#table1">Table 1</a>. These transformations are designed to isolate and test the 
				embedding dimensions most responsive to each property.
			</p>
			<p>
				To evaluate the responsiveness of embedding dimensions to these linguistic properties, we utilized the <b>Embedding Dimension Impact (EDI)</b> metric (<a href="#ref_14">[13]</a>), 
				a measure that quantifies the importance of each embedding dimension for encoding a specific linguistic transformation. The EDI score is computed as:
			</p>
			<p>
				\[
				\text{EDI}_{d,lp} = w_1 \cdot T_{d,lp} + w_2 \cdot MI_{d,lp} + w_3 \cdot R_{d,lp},
				\]
			</p>
			<p>
				where:
			</p>
			<ul>
				<li><b>\( T_{d,lp} \)</b>: The negative log of the p-value from the statistical pairwise t-test, which determines whether there is a significant 
				difference between the embedding values for Sentence 1 and Sentence 2 in each LDSP.</li>
				<li><b>\( MI_{d,lp} \)</b>: The mutual information score, quantifying the amount of information shared between the embedding dimension and the linguistic property.</li>
				<li><b>\( R_{d,lp} \)</b>: The absolute value of the logistic regression weight for the dimension after recursive feature elimination (RFE). 
				If the dimension is removed during RFE, \( R_{d,lp} = 0 \).</li>
			</ul>
			<p>
				The weights \( w_1, w_2, w_3 \) were optimized in prior work to balance the contributions of each component, ensuring that the EDI metric reliably captures the significance of each dimension for linguistic transformations.
			</p>
			
			<h2>Dataset overview</h2>
			<p>
				The LDSP dataset includes 1000 sentence pairs for each of the 10 linguistic properties we investigate.
				The dataset was generated using Google's <code>gemini-1.5-flash</code> model API. 
			</p>
			
			<h2>Linguistic properties</h2>
			<p>
				The linguistic properties tested were chosen to explore various semantic and syntactic relationships. 
				Below is a summary of each property from the LDSP dataset:
			</p>
			<table border="1" align=center cellspacing="0" cellpadding="10" width="50%" id="table1">
				<thead>
					<tr>
						<th>Linguistic property</th>
						<th>Description</th>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td><i>Control</i></td>
						<td>Contains completely unrelated sentence pairs for comparison.</td>
					</tr>
					<tr>
						<td><i>Synonym</i></td>
						<td>Both sentences have the same meaning, with one word replaced by its synonym.</td>
					</tr>
					<tr>
						<td><i>Quantity</i></td>
						<td>Switch from an exact number to a grouping word.</td>
					</tr>
					<tr>
						<td><i>Tense</i></td>
						<td>One sentence is constructed in the present tense, while the other is in the past tense.</td>
					</tr>
					<tr>
						<td><i>Intensifier</i></td>
						<td>Degree of emphasis present within a sentence.</td>
					</tr>
					<tr>
						<td><i>Definiteness</i></td>
						<td>Use of definite or indefinite articles within a sentence, such as "the" compared to "a".</td>
					</tr>
					<tr>
						<td><i>Factuality</i></td>
						<td>The degree of truth implied by the structure of the sentence.</td>
					</tr>
					<tr>
						<td><i>Polarity</i></td>
						<td>Similar to negation but occurs when an antonym is used to reverse the meaning of the sentence completely.</td>
					</tr>
					<tr>
						<td><i>Negation</i></td>
						<td>Addition of "not" to a sentence, negating the meaning.</td>
					</tr>
				</tbody>
				
			</table>
			<p style="text-align: center; font-size: 14px">Table 1. Linguistic property descriptions.</p>
			
			
			<br>
			<p>
				These linguistic properties were selected to provide a comprehensive view of both semantic and syntactic relationships in embedding spaces.
				Additionally, the inclusion of the <i>control</i> category ensures that results are contextualized against unrelated sentences, offering a baseline for comparison.
				Our interactive graphic in <a href="#figure2">Figure 2</a> shows examples of Sentence 1 and Sentence 2 according to their linguistic relationship.
			</p>
			<div class="image-container" id="figure2">
				<div class="image-wrapper" data-coordinates="
				752,182,1167,259,image1.jpg;
				752,262,1167,339,image2.jpg;
				752,342,1167,419,image3.jpg;
				752,422,1167,499,image4.jpg;
				752,502,1167,579,image5.jpg;
				752,582,1167,659,image6.jpg;
				752,662,1167,739,image7.jpg;
				752,742,1167,819,image8.jpg;
				752,822,1167,899,image9.jpg;
				752,902,1167,979,image10.jpg">
				<img src="images/linguistic_prop/linguistic_prop.001.jpeg" alt="Default">
				<img src="images/linguistic_prop/linguistic_prop.002.jpeg" alt="Image 1">
				<img src="images/linguistic_prop/linguistic_prop.003.jpeg" alt="Image 2">
				<img src="images/linguistic_prop/linguistic_prop.004.jpeg" alt="Image 3">
				<img src="images/linguistic_prop/linguistic_prop.005.jpeg" alt="Image 4">
				<img src="images/linguistic_prop/linguistic_prop.006.jpeg" alt="Image 5">
				<img src="images/linguistic_prop/linguistic_prop.007.jpeg" alt="Image 6">
				<img src="images/linguistic_prop/linguistic_prop.008.jpeg" alt="Image 7">
				<img src="images/linguistic_prop/linguistic_prop.009.jpeg" alt="Image 8">
				<img src="images/linguistic_prop/linguistic_prop.010.jpeg" alt="Image 9">
				<img src="images/linguistic_prop/linguistic_prop.011.jpeg" alt="Image 10">
				</div>
				<div class="margin-right-block">
					Figure 2. Linguistic property examples.
				</div>
			</div>
			
			<a href="#figure3">Figure 3</a> illustrates the embedding dimensions with high EDI scores.
			On the left, a table of our linguistic 
			properties is displayed. 
			On the right, we plot the embedding dimensions where EDI scores exceed the 
			threshold of 0.8 for the given property. Each vertical line represents a dimension significantly impacted 
			by the linguistic property, indicating that these dimensions are the most responsive to variations 
			involving emphasis within sentences. This way, we can identify which specific dimensions are most important for encoding
			linguistic changes.
		

			<div class="image-container" id="figure3">
				<div class="image-wrapper" data-coordinates="
				70,182,485,259,image1.jpg;
				70,262,485,339,image2.jpg;
				70,342,485,419,image3.jpg;
				70,422,485,499,image4.jpg;
				70,502,485,579,image5.jpg;
				70,582,485,659,image6.jpg;
				70,662,485,739,image7.jpg;
				70,742,485,819,image8.jpg;
				70,822,485,899,image9.jpg;
				70,902,485,979,image10.jpg">
				<img src="images/edi_hover/edi_hover.000.jpeg" alt="Default">
				<img src="images/edi_hover/edi_hover.001.jpeg" alt="Image 1">
				<img src="images/edi_hover/edi_hover.002.jpeg" alt="Image 2">
				<img src="images/edi_hover/edi_hover.003.jpeg" alt="Image 3">
				<img src="images/edi_hover/edi_hover.004.jpeg" alt="Image 4">
				<img src="images/edi_hover/edi_hover.005.jpeg" alt="Image 5">
				<img src="images/edi_hover/edi_hover.006.jpeg" alt="Image 6">
				<img src="images/edi_hover/edi_hover.007.jpeg" alt="Image 7">
				<img src="images/edi_hover/edi_hover.008.jpeg" alt="Image 8">
				<img src="images/edi_hover/edi_hover.009.jpeg" alt="Image 9">
				<img src="images/edi_hover/edi_hover.010.jpeg" alt="Image 10">
				<!-- <div class="margin-right-block">
					Figure 2. Methods diagram; see <a href="#data">Data</a> and <a href="#experiments">Experiments</a> for more information.
				</div> -->
				</div>
				<div class="margin-right-block">
					Figure 3. Dimensions with high EDI scores.
				</div>
			</div>

			</div>
			

		</div>
		

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block" id="experiments">
            <h1>Experiments</h1>
			The task of generating Sentence 2 embeddings based on Sentence 1 embeddings involves modeling the shifts observed in embedding dimensions as they encode linguistic transformations. Two approaches are employed in this project: mean-shift and regression-based transformations. These methods rely on training data (80% of the dataset, 10% validation chunks) to identify patterns and apply those transformations to the test data (remaining 20%). <a href="#figure1">Figure 1</a> details the overall methods diagrams, including the steps to each experiment.
			 
			<div class="sub-content-block" id="sampling">
				<h2>Sampling</h2>
				<p>
					The sampling-based method selectively alters certain dimensions based on their 
					EDI scores. This method ensures that only the most linguistically significant dimensions (as determined by their EDI scores) 
					are modified, while the less significant dimensions remain unchanged. This approach leverages the statistical properties of Sentence 2 embeddings 
					to generate more linguistically meaningful transformations.
				</p>
				<!-- <h3>Methodology</h3> -->
				<p>
					For each dimension in the embedding, its EDI score is compared against a predefined threshold (default is 0.75). Dimensions with EDI scores below the threshold 
					are left unchanged, while those with EDI scores greater than or equal to the threshold are sampled from a Gaussian distribution derived from the 
					corresponding dimension's mean and standard deviation in Sentence 2 embeddings.
				</p>
				<p>
					This selective approach means that we still incorporate the original Sentence 1 embedding into the transformation process. If we were to sample for all dimensions, the original Sentence 1 embedding would not contribute to the generation of Sentence 2 embeddings, defeating the purpose of using Sentence 1 as a reference.
				</p>
				<!-- <h3>Mathematical representation</h3> -->
				<p>
					For a given embedding \( e \), the transformed embedding \( e' \) is generated as follows:
				</p>
				<p>
					\[
					e'[d] = 
					\begin{cases} 
						e[d], & \text{if } \text{EDI}[d] < \text{threshold} \\
						\text{Sample from } \mathcal{N}(\mu_{s2}[d], \sigma_{s2}[d]), & \text{if } \text{EDI}[d] \geq \text{threshold}
					\end{cases}
					\]
				</p>
				<p>
					Here:
				</p>
				<ul>
					<li>\( e[d] \): The \( d \)-th dimension of the original embedding.</li>
					<li>\( \text{EDI}[d] \): The EDI score for the \( d \)-th dimension.</li>
					<li>\( \mathcal{N}(\mu_{s2}[d], \sigma_{s2}[d]) \): Gaussian distribution with mean \( \mu_{s2}[d] \) and standard deviation \( \sigma_{s2}[j] \) 
					derived from Sentence 2 embeddings.</li>
					<li>\( \text{threshold} \): The EDI score threshold.</li>
				</ul>
				<!-- <h3>Significance</h3> -->
				<!-- <p>
					This approach highlights the role of highly impactful embedding dimensions in linguistic transformations, while avoiding unnecessary modifications 
					to dimensions with lower EDI scores. By focusing on sampling from the natural distribution of Sentence 2 embeddings, the sampling-based method 
					ensures that the generated embeddings maintain linguistic relevance and statistical integrity.
				</p> -->
			</div>
			
				<div class="sub-content-block" id="mean-shift">
				<h2>Mean-shift</h2>
					<!-- <p> -->
						The mean-shift approach simplifies the problem by assuming that the transformation for a given linguistic property 
						can be modeled as a fixed, dimension-specific shift. These shifts are calculated from the training data and then 
						applied to test data.
					<!-- </p> -->
					<ol>
						<li>
							<h3>Calculating mean shifts</h3>
							<p>
								For each dimension \( d \) in the embedding space, the mean shift \( \Delta_d \) is calculated as the difference 
								between the average value of that dimension for Sentence 2 embeddings and Sentence 1 embeddings in the training data.
							</p>
							<p>Mathematically:</p>
							<p>
								\[
								\Delta_d = \frac{1}{n} \sum_{i=1}^n E_2[i, d] - \frac{1}{n} \sum_{i=1}^n E_1[i, d]
								\]
								<!-- <span class="highlight-text">This represents the mean shift for dimension \( d \).</span> -->
							</p>
							<p>Here:</p>
							<ul>
								<li>
									\( E_1[i, d] \): Represents the \( d \)-th dimension of the \( i \)-th Sentence 1 embedding.
								</li>
								<li>
									\( E_2[i, d] \): Represents the \( d \)-th dimension of the \( i \)-th Sentence 2 embedding.
								</li>
								<li>
									\( n \): Number of sentences in the training set.
								</li>
							</ul>
							<p>
								The result is a vector \( \Delta = [\Delta_1, \Delta_2, \dots, \Delta_d] \), where \( d \) is the embedding dimensionality.
							</p>
						</li>
						<li>
							<h3>Applying mean shifts</h3>
							<p>
								To generate the embedding for Sentence 2 during testing, the calculated shifts \( \Delta \) are added to the corresponding
								dimensions of the Sentence 1 embedding:
							</p>
							<p>
								\[
								e_2'[d] = e_1[d] + \Delta_d \quad \forall d \in \{1, \dots, D\}
								\]
							</p>
						</li>
					</ol>
					<p>
						This method is efficient and captures global trends in dimension shifts, making it particularly useful for transformations 
						where shifts are consistent across examples.
					</p>
				</div>

				<div class="sub-content-block" id="regression">
					<h2>Regression</h2>
					<p>
						While the mean-shift approach captures global trends, it does not account for variability or linear relationships within specific dimensions. 
        				The regression-based approach overcomes this limitation by learning dimension-specific regression models to predict changes.
					</p>
					<ol>
						<li>
							<h3>Learning dimension transformations</h3>
							<p>
								For each dimension \( d \), a linear regression model is trained using the \( d \)-th dimension of Sentence 1 embeddings as input 
								and the corresponding dimension of Sentence 2 embeddings as output:
							</p>
							<p>
								\[
								E_2[i, d] = f_d(E_1[i, d]) + \epsilon
								\]
							</p>
							<p>
								where \( f_d(x) = w_d x + b_d \) is a linear function parameterized by the weight \( w_d \) and bias \( b_d \), and \( \epsilon \) is the residual error.
							</p>
							<p>
								The parameters \( w_d \) and \( b_d \) are learned by minimizing the mean squared error (MSE) over the training set:
							</p>
							<p>
								\[
								\min_{w_d, b_d} \frac{1}{n} \sum_{i=1}^n \left( E_2[i, d] - (w_d E_1[i, d] + b_d) \right)^2
								\]
							</p>
						</li>
						<li>
							<h3>Generating regression-based embeddings</h3>
							<p>
								For a test embedding \( e_1 \), the regression models are applied to each dimension to predict the corresponding dimensions of Sentence 2:
							</p>
							<p>
								\[
								e_2'[d] = f_d(e_1[d]) = w_d e_1[d] + b_d \quad \forall d \in \{1, \dots, D\}
								\]
							</p>
						</li>
					</ol>
					<p>
						This approach allows each dimension to respond differently to the linguistic transformation, making it more flexible than the mean-shift method.
					</p>
				</div>

				<div class="sub-content-block" id="edi-cosine-loss">
					<h2>EDI + Cosine loss</h2>
					<p>
						The EDI + Cosine loss method introduces a weighted transformation loss that selectively penalizes changes to embedding dimensions based on 
						their EDI scores. By combining a dimension-specific weighting mechanism with cosine similarity, this method 
						ensures that embedding transformations preserve linguistic meaning while penalizing unnecessary deviations.
					</p>
					<ol>
						<li>
							<h3>Methodology</h3>
							<p>
								For each dimension \( d \) in the embedding, the EDI score determines its contribution to the transformation loss. Dimensions with high EDI scores 
								are considered linguistically significant and are assigned lower penalties for changes, encouraging the model to focus on altering dimensions 
								that encode meaningful transformations. Dimensions with low EDI scores, on the other hand, are penalized more heavily to prevent unnecessary changes.
							</p>
							<p>
								The EDI scores are scaled using a sigmoid function, defined as:
							</p>
							<p>
								\[
								\text{EDI}_d = \text{Sigmoid}(5 \cdot (\text{EDI}[d] - 0.5))
								\]
							</p>
							<p>
								Here:
							</p>
							<ul>
								<li>\( \text{EDI}[d] \): The EDI score for the \( d \)-th dimension.</li>
								<li>\( \text{EDI}_d \): The scaled weight for the \( d \)-th dimension, emphasizing high EDI scores and de-emphasizing low scores.</li>
							</ul>
							<br>
						</li>
						<li>
							<h3>Loss calculation</h3>
							<p>
								The EDI + Cosine Loss combines dimension-specific penalties and a similarity measure. The total loss is computed as:
							</p>
							<p>
								\[
								\mathcal{L} = \frac{1}{D} \sum_{d=1}^D \left( 1 - \text{EDI}_d \right) \cdot |e_1[d] - e_2[d]| \cdot (1 - \text{sim}(e_1, e_2))
								\]
							</p>
							<p>
								Here:
							</p>
							<ul>
								<li>\( d \): The total number of dimensions in the embedding.</li>
								<li>\( e_1[d] \): The \( d \)-th dimension of the original embedding (Sentence 1).</li>
								<li>\( e_2[d] \): The \( d \)-th dimension of the transformed embedding (Sentence 2).</li>
								<li>\( \text{EDI}_d \): The scaled EDI score for the \( d \)-th dimension, derived using the chosen scaling method.</li>
								<li>\( \text{sim}(e_1, e_2) \): The <i>cosine similarity</i> between the original and transformed embeddings, i.e., cosine distance between two vectors the cosine distance between two vectors.</li>
							</ul>
							<p>
								By combining the weighted dimension-wise changes and cosine similarity, this loss function encourages transformations that preserve 
								overall semantic similarity while prioritizing changes in dimensions with high EDI scores.
							</p>
						</li>
					</ol>
					<p>
						The EDI + Cosine Loss method is particularly effective in tasks where preserving the semantic meaning of embeddings is critical. 
						By weighting dimensions based on their linguistic significance (EDI scores), the transformations are both targeted and interpretable. 
						Furthermore, the integration of cosine similarity aligns the embeddings globally, enabling the method to balance local changes with overall structural coherence.
					</p>
				</div>

				<div class="sub-content-block" id="contrastive-loss">
					<h2>Contrastive loss methods</h2>
					<p>
						The contrastive loss methods introduce a custom objective function that evaluates embedding transformations by comparing them against both positive and negative pairs. 
						The goal is to minimize the distance between the transformed embedding and the original embedding (positive pair) while maximizing the distance between 
						the transformed embedding and a randomly sampled negative embedding (negative pair). These methods encourage meaningful transformations that preserve 
						semantic consistency while separating embeddings from unrelated contexts.
					</p>
					<ol>
						<li>
							<h3>Contrastive loss objective</h3>
							<p>
								For a transformed embedding \( e_2' \), the contrastive loss is defined as:
							</p>
							<p>
								\[
								\mathcal{L}_{\text{contrastive}} = \text{dist}(e_2', e_1) + \max \left( 0, \text{margin} - \text{dist}(e_2', e_{\text{neg}}) \right)
								\]
							</p>
							<p>
								Here:
							</p>
							<ul>
								<li>\( e_2' \): The transformed embedding (Sentence 2).</li>
								<li>\( e_1 \): The original embedding (Sentence 1).</li>
								<li>\( e_{\text{neg}} \): A randomly sampled negative embedding (unrelated sentence).</li>
								<li>\( \text{margin} \): A predefined margin for sufficient separation from negative examples (default is 1).</li>
								<li>\( \text{dist}(x, y) \): A distance function that measures the similarity or dissimilarity between two embeddings.</li>
							</ul>
						
							<p>The distance function \( \text{dist}(x, y) \) can be one of the following:</p>
							<ul>
								<li><b>Mean squared error (MSE):</b> Measures the squared Euclidean distance:
									\[
									\text{dist}(x, y) = \|x - y\|^2
									\]
								</li>
								<li><b>Cosine similarity:</b> Measures the cosine distance between two vectors:
									\[
									\text{dist}(x, y) = 1 - \text{sim}(x, y)
									\]
								</li>
							</ul>
						</li>
						<li>
							<h3>Combined loss</h3>
							<p>
								For robust embedding transformations, the contrastive loss can be combined with the transformation loss as follows:
							</p>
							<p>
								\[
								\mathcal{L} = \lambda_1 \cdot \mathcal{L}_{\text{transformation}} + \lambda_2 \cdot \mathcal{L}_{\text{contrastive}},
								\]
							</p>
							<p>
								where \( \lambda_1 \) = 1 = \( \lambda_2 \), i.e., the losses are equally weighted.
							</p>
						</li>
					</ol>
					<!-- <h3>Significance</h3> -->
					<p>
						The contrastive loss methods provide an additional layer of robustness by explicitly introducing a negative sampling objective. 
						By minimizing the distance to the original embedding and maximizing the distance from negative examples, we aim to have 
						transformed embeddings maintain their semantic relevance while remaining distinguishable from unrelated contexts. The combined 
						loss further integrates these objectives, balancing the preservation of linguistic properties with enhanced robustness against noise.
					</p>
				</div>
				
					

			
			<!-- <ol>
				<li>Mean-shift</li>
				<li>Regression</li>
				<li>Milk</li>
			</ol> -->

            <!-- In this section we embed a video:
						<video class='my-video' loop autoplay muted style="width: 725px">
								<source src="./images/mtsh.mp4" type="video/mp4">
						</video> -->
		    </div>
		    <!-- <div class="margin-right-block">
					A caption for the video could go here.
		    </div> -->
		</div>

	</div>

	<div class="content-margin-container" id="training">
		<div class="margin-left-block">
		</div>
	<div class="main-content-block">
		<h1>Training</h1>
		<p>
			In the <a href="#experiments">Experiments</a> section, we proposed 
			multiple methods to generate Sentence 2 embeddings from Sentence 1 embeddings. Each of these methods (mean shift, regression, sampling, contrastive losses) 
			hypothesizes a different mechanism to encode linguistic transformations. The EDI-weighted generator is used to test and compare these methods in a unified framework 
			by learning transformations directly influenced by embedding dimensionality.
		</p>
	
		<h2>Generator model</h2>
		<p>
			The generator model is a feedforward neural network designed to transform high-dimensional embeddings. It consists of:
		</p>
		<ul>
			<li><b>Input layer:</b> Accepts embeddings of dimension \( D \), where \( D \) is the size of the embedding vector.</li>
			<li><b>Hidden layer:</b> A fully connected layer with a ReLU activation function to introduce non-linearity.</li>
			<li><b>Output layer:</b> A fully connected layer that outputs transformed embeddings of the same dimension \( D \).</li>
		</ul>
		<p>
			Mathematically, the transformation can be expressed as:
		</p>
		<p>
			\[
			\mathbf{e}_2' = \sigma(\mathbf{W}_2 \cdot \text{ReLU}(\mathbf{W}_1 \cdot \mathbf{e}_1 + \mathbf{b}_1) + \mathbf{b}_2)
			\]
		</p>
		<p>
			Here:
		</p>
		<ul>
			<li>\( \mathbf{e}_1 \): Input embedding (Sentence 1).</li>
			<li>\( \mathbf{e}_2' \): Transformed embedding (predicted Sentence 2).</li>
			<li>\( \mathbf{W}_1, \mathbf{W}_2 \): Weight matrices for the hidden and output layers, respectively.</li>
			<li>\( \mathbf{b}_1, \mathbf{b}_2 \): Bias vectors for the hidden and output layers, respectively.</li>
			<li>\( \sigma \): Activation function (identity in this case).</li>
		</ul>
	
		<h2>Custom objective function</h2>
		<p>
			The training process uses the <b>EDI-weighted transformation loss</b>, which combines an EDI-weighted dimension-wise loss and a cosine similarity objective. 
			This objective function ensures that the generator:
		</p>
		<ul>
			<li>Prioritizes embedding dimensions with higher EDI scores for linguistic transformations.</li>
			<li>Encodes transformations while preserving the overall semantic structure of the embedding space.</li>
			<li>Discourages overfitting by incorporating a contrastive loss term with negative examples.</li>
		</ul>
		<p>
			The total loss for a single transformed embedding is given by:
		</p>
		<p>
			\[
			\mathcal{L} = \underbrace{\frac{1}{D} \sum_{d=1}^D \left(1 - \text{EDI}_d\right) \cdot \left| e_{1,d} - e_{2,d}' \right|}_{\text{EDI-weighted dimension loss}}
			+ \lambda \cdot \left(1 - \text{sim}(\mathbf{e}_2', \mathbf{e}_2)\right),
			\]
		</p>
		<p>
			where:
		</p>
		<ul>
			<li>\( \text{EDI}_d \): EDI score for dimension \( d \).</li>
			<li>\( e_{1,d} \): Dimension \( d \) of the original Sentence 1 embedding.</li>
			<li>\( e_{2,d}' \): Dimension \( d \) of the predicted Sentence 2 embedding.</li>
			<li>\( \text{sim}(\mathbf{e}_2', \mathbf{e}_2) \): Cosine similarity between the predicted and ground-truth embeddings.</li>
			<li>\( \lambda \): Weighting factor for the cosine similarity loss term.</li>
		</ul>
	
		<h2>Training procedure</h2>
		<p>
			The generator is trained over multiple epochs using the Adam optimizer, with the following key steps:
		</p>
		<ol>
			<li><b>Batch sampling:</b> Training embeddings are divided into batches of size \( B \), where each batch contains \( \mathbf{e}_1 \) and \( \mathbf{e}_2 \) pairs.</li>
			<li><b>Negative sampling:</b> Negative examples are generated by randomly shuffling embeddings within each batch to create unaligned pairs.</li>
			<li><b>Forward pass:</b> The generator predicts transformed embeddings \( \mathbf{e}_2' \) from \( \mathbf{e}_1 \).</li>
			<li><b>Loss calculation:</b> The EDI-weighted loss is computed for each batch, combining transformation and contrastive loss terms.</li>
			<li><b>Backward pass:</b> Gradients of the loss with respect to the model parameters are computed and used to update the weights using the Adam optimizer.</li>
		</ol>
	
		By the end of training, the generator learns to produce embeddings that effectively capture the semantic and syntactic transformations in the LDSP dataset. 

	</div>
	<!-- <div class="margin-right-block">
	</div> -->
</div>

		<div class="content-margin-container" id="eval">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
					<h1>Evaluation</h1>
					<p>
						To assess the effectiveness of embedding generation methods, we evaluated the generated Sentence 2 embeddings against the ground-truth Sentence 2 embeddings 
						and baseline Sentence 1 embeddings using <b>cosine similarity</b> and <b>Euclidean distance</b>. These metrics provide a consistent measure of alignment for all methods.
					</p>
				
					<p>
						The evaluation compared the following methods:
					</p>
					<ul>
						<li>Baseline (Sentence 1 embeddings)</li>
						<li>Mean Shift</li>
						<li>Regression</li>
						<li>Sampling</li>
						<li>EDI + Cosine Loss</li>
						<li>Contrastive MSE Loss</li>
						<li>Contrastive Cosine Loss</li>
					</ul>
					<p>
						For each method, similarities were calculated between the generated and ground-truth embeddings. These values were then analyzed to determine 
						how effectively each method captured linguistic transformations.
					</p>
		</div>
		<!-- <div class="margin-right-block">
		</div> -->
	</div>

	<div class="content-margin-container" id="results">
		<div class="margin-left-block">
		</div>
	<div class="main-content-block">
				<h1>Results</h1>
				<p>
					Cosine similarity and Mean Squared Error (MSE) provide complementary perspectives for evaluating embedding generation techniques, 
					with their utility varying depending on the linguistic property under consideration.
				</p>
				<ul>
					<li>
						<b>Cosine similarity:</b> Evaluates the angular alignment between the generated embedding and the ground-truth embedding, focusing on direction rather than magnitude. 
						It is particularly helpful for transformations involving <i>semantic shifts</i>, such as <b>synonymy</b> or <b>negation</b>, where preserving meaning is critical. 
						High cosine similarity indicates that the generated embedding maintains the semantic relationships of the ground truth.
					</li>
					<br>
					<li>
						<b>Euclidean distance</b>: Measures the absolute difference between the generated embedding and the ground-truth embedding, emphasizing magnitude preservation. 
						It is particularly useful for <i>structural transformations</i>, such as <b>tense</b> or <b>voice</b>, where specific patterns of embedding changes correlate strongly with the transformation. Low Euclidean distance indicates quantitative alignment with the ground truth.
					</li>
				</ul>
				<p>
					By combining these metrics, we gain a nuanced understanding of embedding performance. 
					Cosine similarity highlights semantic fidelity, while Euclidean distance targets structural accuracy.
				</p>
				<div class="image-container" id="figure4">
					<div class="image-wrapper" data-coordinates="
					70,182,485,259,image1.jpg;
					70,262,485,339,image2.jpg;
					70,342,485,419,image3.jpg;
					70,422,485,499,image4.jpg;
					70,502,485,579,image5.jpg;
					70,582,485,659,image6.jpg;
					70,662,485,739,image7.jpg;
					70,742,485,819,image8.jpg;
					70,822,485,899,image9.jpg;
					70,902,485,979,image10.jpg">
					<img src="images/eval_plots_hover/eval_plots_hover.000.jpeg" alt="Default">
					<img src="images/eval_plots_hover/eval_plots_hover.002.jpeg" alt="Image 1">
					<img src="images/eval_plots_hover/eval_plots_hover.003.jpeg" alt="Image 2">
					<img src="images/eval_plots_hover/eval_plots_hover.004.jpeg" alt="Image 3">
					<img src="images/eval_plots_hover/eval_plots_hover.005.jpeg" alt="Image 4">
					<img src="images/eval_plots_hover/eval_plots_hover.006.jpeg" alt="Image 5">
					<img src="images/eval_plots_hover/eval_plots_hover.007.jpeg" alt="Image 6">
					<img src="images/eval_plots_hover/eval_plots_hover.008.jpeg" alt="Image 7">
					<img src="images/eval_plots_hover/eval_plots_hover.009.jpeg" alt="Image 8">
					<img src="images/eval_plots_hover/eval_plots_hover.010.jpeg" alt="Image 9">
					<img src="images/eval_plots_hover/eval_plots_hover.011.jpeg" alt="Image 10">
					</div>
					<div class="margin-right-block">
						Figure 4. Boxplots displaying cosine similarity and Euclidean distance scores across linguistic properties
					</div>
				</div>
    <h2>General trends across methods</h2>
    <ul>
        <li>
            <b>Baseline (Sentence 1 embedding) vs. generated embeddings:</b> 
            Across all linguistic properties, baseline embeddings consistently achieve lower cosine similarity compared to generated embeddings, 
            confirming the necessity of explicit embedding generation methods for encoding linguistic transformations. 
            Baseline embeddings also exhibit notably higher MSE, indicating their inability to capture quantitative and structural changes.
        </li>
		<br>
        <li>
            <b>Performance of specific methods:</b> 
            <ul>
                <li><b>Regression-based generation</b> consistently outperforms simpler methods (e.g., sampling or mean-shift) across most linguistic properties, highlighting its ability to capture linear relationships in embedding transformations.</li>
                <li><b>Contrastive methods</b> (e.g., Contrastive Cosine Loss, Contrastive MSE Loss, etc.) perform well for <i>semantic transformations</i> like synonymy and negation, showcasing robustness in preserving semantic fidelity while penalizing divergence from unrelated embeddings.</li>
                <li><b>Mean-shift</b> demonstrates moderate performance for properties involving small, uniform changes (e.g., definiteness) but struggles with more complex transformations.</li>
            </ul>
        </li>
    </ul>

	<img src="images/mse_table.jpeg" width="70%" id="figure1">
	<p style="text-align: center; font-size: 14px">Table 2. 95% confidence intervals of MSE scores across linguistic properties and experiments.</p>
	<br>
    
    <h2>Linguistic property-specific insights</h2>
    <ul>
        <li>
            <b>Control:</b> For unrelated sentence pairs, cosine similarity is low across all methods, as expected. Generated embeddings exhibit limited alignment with ground-truth embeddings, indicating no linguistic relationship exists to guide the transformation.
        </li>
        <li>
            <b>Negation:</b> Cosine similarity excels for negation, where angular relationships between embeddings are key to capturing polarity shifts. 
            Regression and contrastive methods achieve the highest scores, demonstrating their ability to handle semantic inversions effectively.
        </li>
        <li>
            <b>Tense and Voice:</b> MSE is more informative for structural transformations like tense and voice, as these involve predictable shifts in embedding magnitudes. 
            Low MSE scores reflect how closely the generated embeddings align structurally with the ground truth.
        </li>
        <li>
            <b>Definiteness and Quantity:</b> Both definiteness (e.g., "a" vs. "the") and quantity (specific numbers to grouping words) exhibit moderate performance across methods. 
            Cosine similarity highlights semantic fidelity, while MSE provides insights into the precision of structural alignment.
        </li>
        <li>
            <b>Synonymy:</b> Cosine similarity is particularly valuable for synonymy, as it evaluates whether the semantic equivalence between the original and transformed sentences is preserved. 
            Contrastive methods achieve the highest cosine similarity scores, demonstrating their effectiveness in handling lexical variation.
        </li>
        <li>
            <b>Intensifier and Polarity:</b> Both properties show robust performance across contrastive and regression methods. Improvements in cosine similarity over the baseline indicate these methods effectively capture semantic emphasis (intensifier) and polarity shifts.
        </li>
    </ul>
    
    <h2>Method-specific observations</h2>
    <ul>
        <li><b>Regression Methods:</b> These demonstrate strong alignment with ground-truth embeddings across a wide range of linguistic properties, making them a versatile option for embedding generation.</li>
        <li><b>Contrastive Loss:</b> These methods exhibit robustness across both semantic and structural transformations, achieving consistently high cosine similarity and low variance, particularly for complex properties like synonymy and polarity shifts.</li>
        <li><b>Mean-shift:</b> While effective for uniform shifts (e.g., definiteness), this simpler method struggles with more complex linguistic transformations.</li>
    </ul>
    
	Our primary insight is that <i>contrastive loss</i> and <i>regression</i> methods consistently outperform simpler methods, demonstrating their ability to handle both semantic and structural linguistic properties effectively.

	</div>
	<!-- <div class="margin-right-block">
	</div> -->
</div>

		<div class="content-margin-container" id="discussion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Discussion</h1>
						<p>
							This work represents a significant step forward in the study of sentence embeddings by 
							advancing the understanding and generation of embeddings for linguistic transformations. 
							Current research in natural language processing (NLP) often emphasizes static embeddings 
							or probing methods that reveal latent properties, yet fails to address how dynamic linguistic 
							relationships are encoded or manipulated. Our generative approach fills this critical gap by 
							focusing on producing embeddings for Sentence 2 directly from Sentence 1, based on specific 
							linguistic transformations. This enables a deeper exploration of how semantic and syntactic 
							changes are represented within high-dimensional spaces, fitting seamlessly into the growing 
							body of research on interpretability and embedding manipulation.
						</p>
						<p>
							Understanding and generating embeddings for linguistic transformations are vital for real-world 
							applications where adaptability and precision are crucial. Tasks like controlled text generation, 
							paraphrasing, and personalization in NLP systems benefit directly from the ability to encode 
							linguistic properties dynamically. Furthermore, these insights contribute to explainable AI by 
							offering transparency into how models handle and manipulate semantic relationships, bridging the 
							gap between theoretical research and practical applications.
						</p>
						<p>
							Our main contributions to this field include:
						</p>
						<ul>
							<li><i>Embedding generation methods</i>: We developed and compared several approaches to generating 
								Sentence 2 embeddings, such as mean shift, sampling, regression, and contrastive loss-based techniques. 
								These methods provide diverse tools for analyzing and manipulating embeddings.
							</li>
							<li>
								<i>Evaluation framework</i>: We designed an evaluation process that compares generated embeddings to ground-truth embeddings using cosine similarity, offering a clear measure of alignment and performance across methods.
							</li>
						</ul>
						<p>
							The implications of this work are broad, spanning both theoretical and practical domains. 
							Theoretically, it highlights the importance of embedding dynamics in understanding linguistic 
							transformations, paving the way for further advancements in interpretability and embedding design. 
							Practically, the ability to generate and evaluate embeddings equips NLP systems with fine-grained 
							control over linguistic properties, enhancing their robustness and adaptability in real-world applications. 
							These findings also lay the groundwork for future studies on combining linguistic properties, adapting 
							embeddings for specific tasks, and extending evaluation methods to include task-specific metrics.
						</p>
						<p>
							By addressing the underexplored area of generative embeddings and their evaluation, this work contributes 
							to a richer understanding of how sentence embeddings operate and interact with linguistic transformations. 
							This not only strengthens the theoretical foundation of embedding research but also provides actionable 
							insights for building more interpretable and effective NLP systems in a rapidly evolving landscape.
						</p>
						<h2>Limitations</h2>
    <p>
        While this study provides valuable insights into generating embeddings for linguistic transformations, it also open avenues for future work:
    </p>
	<ul>
		<li><i>Robustness to linguistic variability</i>: Our evaluation focused on distinct, predefined linguistic properties. 
			However, the robustness of our methods to more nuanced or combined properties — such as the addition of modifiers like adjectives, for example — remains untested. 
			Additionally, the sentences analyzed in this study are relatively simple and focus on one linguistic 
			property at a time. Exploring embeddings generated for more complex sentences with multiple intertwined properties 
			could reveal additional strengths or limitations of our approach.
		</li>
		<!-- <br> -->
		<li><i>Task-agnostic evaluation</i>: While our evaluation methods focus on alignment with ground-truth embeddings, 
			they do not address performance in downstream tasks, such as classification, machine translation, or sentiment analysis. 
			This task-agnostic approach ensures broad applicability, but understanding task-specific utility would provide a 
			more comprehensive evaluation of these methods.</li>
			<!-- <br> -->
		<li><i>Generator model architecture</i>: While our current generator model is sufficient for capturing key insights, more sophisticated architectures, such as transformers or variational autoencoders, may improve the quality of generated embeddings. 
			However, given the primary focus of this work on embedding alignment and interpretability, the current architectures and evaluation methods provide a sufficient foundation for our conclusions.
		</li>
	</ul>
 
		    </div>
		    <!-- <div class="margin-right-block">
		    </div> -->
		</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] Devlin, J., Chang, M.W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 4171-4186). Association for Computational Linguistics. <a href="https://arxiv.org/abs/1810.04805">[link]</a><br><br>
							<a id="ref_2"></a>[2] Murphy, B., Talukdar, P. P., & Mitchell, T. (2012, December). Learning effective and interpretable semantic models using non-negative sparse embedding. In International Conference on Computational Linguistics (COLING 2012), Mumbai, India (pp. 1933-1949). Association for Computational Linguistics. <a href="https://aclanthology.org/C12-1118.pdf">[link]</a><br><br>
							<a id="ref_3"></a>[3] Sun, F., Guo, J., Lan, Y., Xu, J., & Cheng, X. (2016, July). Sparse word embeddings using l1 regularized online learning. In Twenty-Fifth International Joint Conference on Artificial Intelligence. <a href="https://www.ijcai.org/Proceedings/16/Papers/414.pdf">[link]</a><br><br>
							<!-- <a id="ref_4"></a>[4] Han, J., Lu, Z., & Xu, Y. (2012). Non-Negative Sparse Embeddings for Language Models. EMNLP 2012. <a href="https://aclanthology.org/C12-1118.pdf">[link]</a><br><br> -->
							<a id="ref_5"></a>[4] Faruqui, M., Tsvetkov, Y., Yogatama, D., Dyer, C., & Smith, N. (2015). Sparse overcomplete word vector representations. arXiv preprint arXiv:1506.02004. <a href="https://arxiv.org/abs/1506.02004">[link]</a><br><br>
							<a id="ref_6"></a>[5] Subramanian, A., Pruthi, D., Jhamtani, H., Berg-Kirkpatrick, T., & Hovy, E. (2018, April). SPINE: SParse interpretable neural embeddings. In Proceedings of the AAAI conference on artificial intelligence (Vol. 32, No. 1). <a href="https://arxiv.org/abs/1711.08792">[link]</a><br><br>
							<a id="ref_7"></a>[6] Panigrahi, A., Simhadri, H. V., & Bhattacharyya, C. (2019, July). Word2Sense: Sparse interpretable word embeddings. In Proceedings of the 57th annual meeting of the Association for Computational Linguistics (pp. 5692-5705). <a href="http://dx.doi.org/10.18653/v1/P19-1570">[link]</a><br><br>
							<a id="ref_8"></a>[7] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., & Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364. <a href="https://aclanthology.org/D17-1070/">[link]</a><br><br>
							<a id="ref_9"></a>[8] Hewitt, J., & Manning, C. D. (2019, June). A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 4129-4138). <a href="https://aclanthology.org/N19-1419/">[link]</a><br><br>
							<a id="ref_10"></a>[9] Şenel, L. K., Utlu, I., Yücesoy, V., Koc, A., & Cukur, T. (2018). Semantic structure and interpretability of word embeddings. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26(10), 1769-1779. <a href="https://arxiv.org/abs/1711.00331">[link]</a><br><br>
							<a id="ref_11"></a>[10] Simhi, A., & Markovitch, S. (2022). Interpreting embedding spaces by conceptualization. arXiv preprint arXiv:2209.00445. <a href="https://aclanthology.org/2023.emnlp-main.106/">[link]</a><br><br>
							<a id="ref_12"></a>[11] Kerscher, M., & Eger, S. (2020). Vec2Sent: probing sentence embeddings with natural language generation. arXiv:2011.00592. <a href="https://aclanthology.org/2020.coling-main.152/">[link]</a><br><br>
							<a id="ref_13"></a>[12] Huang, J. Y., Yao, W., Song, K., Zhang, H., Chen, M., & Yu, D. (2023). Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations. arXiv:2305.14599. <a href="https://arxiv.org/abs/2305.14599">[link]</a><br><br>
							<a id="ref_14"></a>[13] <a href="https://www.dropbox.com/scl/fi/7jwfuxlgukunlvhgmejxy/disentangling_linguistic_properties.pdf?rlkey=igyt34s0tnhw80lltvlxul7ls&st=46qr9zgy&dl=0">Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings</a><br><br>
						</div>
		    </div>
		    <!-- <div class="margin-right-block">
		    </div> -->
		</div>

	<script src="hoverEffect.js"></script>

	</body>

</html>
